dir_string =  paste0('../processed_data/',d_type,"/",sex_i,"/",age_i,"/",race_i)
# Subset and pepare data save as json
formatted_data = input_d %>%
subset_data(sex_i,age_i,race_i) %>%
cache_densities_and_incomes(1971) %>%
make_jsons_and_save(dir_string)
}
}
}
}
subset_data = function(full_data,sex_string,age_string,race_string){
return(
full_data %>%
subset_sex(sex_string) %>%
subset_age(age_string) %>%
subset_race(race_string)
)
}
subset_sex = function(input_data,sex_string){
if (sex_string == 'all_sexes') return(input_data)
return(subset(input_data,SEX==sex_dict[sex_string]))
}
subset_age = function(input_data,age_string){
if (age_string=='under_30') return(subset(input_data,AGE<31))
if (age_string=='from_30_to_49') return(subset(input_data,AGE>=31 & AGE <= 50))
if (age_string=='50_plus') return(subset(input_data,AGE>=51))
return(input_data)
}
subset_race = function(input_data,race_string){
if (race_string=='white_non_hispanic') return(subset(input_data,RACE==100 & HISPAN == 0))
if (race_string=='black_non_hispanic') return(subset(input_data,RACE==200 & HISPAN == 0))
if (race_string=='hispanic') return(subset(input_data,HISPAN %in% hispanic_list))
return(input_data)
}
# Get the increase weight for bandwidths overlapping with negative values to correct for 0-axis bias
less_than_zero_weights = function(kernel_center,bandwidth){
if (kernel_center - bandwidth >= 0 ){
# Kernel is entirely positive;
return(1)
}
# Kernel overlaps with negative values;
# Return weight that's proportional to the proportion of the kernel that's below zero
# That way, the area of the kernel that's non-negative will be 1
# This is done to mitigate the bias caused by having a discountinuous x axis
term_one = kernel_center+bandwidth
term_two = term_one^3 / (3*(bandwidth^2))
term_three = ((term_one^2) * kernel_center) / (bandwidth^2)
term_four = ((kernel_center^2) * term_one) / (bandwidth^2)
total_sum = term_one - term_two + term_three - term_four
return(1 / (total_sum * (0.75/bandwidth)))
}
mirror_data = function(input_data,bandwidth){
data_to_mirror = subset(input_data, real_income < bandwidth)
data_to_mirror$real_income = data_to_mirror$real_income * (-1)
return(rbind(input_data,data_to_mirror))
}
epanechnikov_kernel = function(bandwidth,threshold,kernel_center,asecwt,mirror_data) {
weighted_difference = abs((threshold - kernel_center) / bandwidth)
if (weighted_difference >= 1) return(0)
zero_weight = if (mirror_data) 1 else less_than_zero_weights(kernel_center,bandwidth)
kernel_weight = zero_weight * asecwt
return(kernel_weight * (0.75 * (1 - (weighted_difference^2)) / bandwidth))
}
estimate_densities = function(input_data,mirror_data){
# Take the sum of the kernels, weight = (ASECWT * less than zero weights)
# Divide all by (ASECWT * less than zero weights)
if (mirror_data) input_data = mirror_data(input_data)
densities = rep(0.0,length(thresholds))
thresholds_to_loop = thresholds[thresholds < max(input_data$real_income) + bandwidth]
densities = sapply(thresholds_to_loop,get_threshold_density,data_for_estimation = input_data,mirror_data = mirror_data) / sum(input_data$ASECWT)
# Append zeros for thresholds above the max in the data
densities = c(densities,rep(0,length(thresholds) - length(thresholds_to_loop)))
return(densities)
}
get_threshold_density = function(threshold,data_for_estimation,mirror_data){
threshold_subset = subset(input_data, abs(real_income-threshold) < bandwidth)
if (nrow(threshold_subset) == 0) return(0)
return(sum(apply(threshold_subset,1,function(d) epanechnikov_kernel(bandwidth,threshold,d['real_income'],d['ASECWT'],mirror_data))))
}
data = load_and_clean_data()
# Worked 50-52 weeks last year. Redefine current subset to minimize memory usage
current_subset = subset(data,WKSWORK2==6)
# Worked full time last year
current_subset = subset(current_subset,FULLPART==1)
input_data = subset(current_data,YEAR==2000)
input_data = subset(current_subset,YEAR==2000)
densities_old = estimate_densities(input_data,T)
plot(densities_old,main='mirrored')
estimate_densities = function(input_data,mirror_data){
# Take the sum of the kernels, weight = (ASECWT * less than zero weights)
# Divide all by (ASECWT * less than zero weights)
if (mirror_data) input_data = mirror_data(input_data,bandiwdth)
densities = rep(0.0,length(thresholds))
thresholds_to_loop = thresholds[thresholds < max(input_data$real_income) + bandwidth]
densities = sapply(thresholds_to_loop,get_threshold_density,data_for_estimation = input_data,mirror_data = mirror_data) / sum(input_data$ASECWT)
# Append zeros for thresholds above the max in the data
densities = c(densities,rep(0,length(thresholds) - length(thresholds_to_loop)))
return(densities)
}
# mirror data
densities_old = estimate_densities(input_data,T)
estimate_densities = function(input_data,mirror_data){
# Take the sum of the kernels, weight = (ASECWT * less than zero weights)
# Divide all by (ASECWT * less than zero weights)
if (mirror_data) input_data = mirror_data(input_data,banwidth)
densities = rep(0.0,length(thresholds))
thresholds_to_loop = thresholds[thresholds < max(input_data$real_income) + bandwidth]
densities = sapply(thresholds_to_loop,get_threshold_density,data_for_estimation = input_data,mirror_data = mirror_data) / sum(input_data$ASECWT)
# Append zeros for thresholds above the max in the data
densities = c(densities,rep(0,length(thresholds) - length(thresholds_to_loop)))
return(densities)
}
# mirror data
densities_old = estimate_densities(input_data,T)
estimate_densities = function(input_data,mirror_data){
# Take the sum of the kernels, weight = (ASECWT * less than zero weights)
# Divide all by (ASECWT * less than zero weights)
if (mirror_data) input_data = mirror_data(input_data,bandwidth)
densities = rep(0.0,length(thresholds))
thresholds_to_loop = thresholds[thresholds < max(input_data$real_income) + bandwidth]
densities = sapply(thresholds_to_loop,get_threshold_density,data_for_estimation = input_data,mirror_data = mirror_data) / sum(input_data$ASECWT)
# Append zeros for thresholds above the max in the data
densities = c(densities,rep(0,length(thresholds) - length(thresholds_to_loop)))
return(densities)
}
# mirror data
densities_old = estimate_densities(input_data,T)
# re-weight data:
densities_new = estimate_densities(input_data,F)
plot(densities_old,main='mirrored')
plot(densities_old,main='reweighted')
estimate_densities = function(input_data,mirror_data){
# Take the sum of the kernels, weight = (ASECWT * less than zero weights)
# Divide all by (ASECWT * less than zero weights)
if (mirror_data) input_data = mirror_data(input_data,bandwidth)
print(nrow(input_data))
densities = rep(0.0,length(thresholds))
thresholds_to_loop = thresholds[thresholds < max(input_data$real_income) + bandwidth]
densities = sapply(thresholds_to_loop,get_threshold_density,data_for_estimation = input_data,mirror_data = mirror_data) / sum(input_data$ASECWT)
# Append zeros for thresholds above the max in the data
densities = c(densities,rep(0,length(thresholds) - length(thresholds_to_loop)))
return(densities)
}
# mirror data
densities_old = estimate_densities(input_data,T)
# re-weight data:
densities_new = estimate_densities(input_data,F)
plot(densities_new,main='reweighted')
plot(densities_old,main='Mirrored')
plot(densities_new,main='Reweighted')
current_subset = subset(data, WKSWORK2 != 9) # Removes missing employment data
current_subset = subset(current_subset, WKSWORK2 != 0) # Removes people who didn't work at all last year (won't have unemployment data on these people as they're NIU)
current_subset$average_weeks_worked = sapply(current_subset$WKSWORK2,get_average_weeks_worked)
current_subset = subset(current_subset,WKSUNEM2 != 8) # Remove missing unemployment current_subset
# Note that there's a discontinuity at 1975/1976
current_subset$average_weeks_unemployed = sapply(current_subset$WKSUNEM2,get_average_weeks_unemployed)
# Only keep people who were in the labor force at least 26 weeks last year
current_subset$total_weeks_in_labor_force = current_subset$average_weeks_worked + current_subset$average_weeks_unemployed
current_subset = subset(current_subset,total_weeks_in_labor_force>=26)
# Export
prepare_data_and_save(current_subset,'mine')
get_average_weeks_worked = function(input){
if (input == 1) return(7)  # 1 - 13 weeks
if (input == 2) return(20) # 14-26 weeks
return(27) # 27+ weeks, don't need to take midpoint as they'll be included automatically
}
get_average_weeks_unemployed = function(input){
if (input==0) return(0)
if (input==1) return (2.5) # 1-4
if (input==2) return (7.5)  # 5 - 10 weeks
if (input==3) return(12.5) # 11- 14 weeks
if (input==4) return(20.5) # 15 - 26 weeks
if (input==9) return(0)
return(27) # 27+ weeks, don't need to take midpoint as they'll be included automatically
}
current_subset = subset(data, WKSWORK2 != 9) # Removes missing employment data
current_subset = subset(current_subset, WKSWORK2 != 0) # Removes people who didn't work at all last year (won't have unemployment data on these people as they're NIU)
current_subset$average_weeks_worked = sapply(current_subset$WKSWORK2,get_average_weeks_worked)
current_subset = subset(current_subset,WKSUNEM2 != 8) # Remove missing unemployment current_subset
# Note that there's a discontinuity at 1975/1976
current_subset$average_weeks_unemployed = sapply(current_subset$WKSUNEM2,get_average_weeks_unemployed)
# Only keep people who were in the labor force at least 26 weeks last year
current_subset$total_weeks_in_labor_force = current_subset$average_weeks_worked + current_subset$average_weeks_unemployed
current_subset = subset(current_subset,total_weeks_in_labor_force>=26)
input_data = subset(current_subset,YEAR==1971)
input_data = subset_data('males','all_ages','white_non_hispanic')
input_data
nrow(input_data)
input_data = subset_data('all_sexes','all_ages','white_non_hispanic')
input_data = data.frame(subset(current_subset,YEAR==1971))
input_data = subset_data('all_sexes','all_ages','white_non_hispanic')
input_data = subset_data(input_data,'all_sexes','all_ages','white_non_hispanic')
hist(input_data$real_income)
# mirror data
densities_old = estimate_densities(input_data,T)
plot(densities_old,main='Mirrored')
# re-weight data:
densities_new = estimate_densities(input_data,F)
plot(densities_new,main='Reweighted')
hist(input_data$real_income)
hist(input_data$real_income,breaks=100)
# mirror data
densities_old = estimate_densities(input_data,T)
plot(densities_old,main='Mirrored')
# re-weight data:
densities_new = estimate_densities(input_data,F)
plot(densities_new,main='Reweighted')
input_data = data.frame(subset(current_subset,YEAR==1972))
input_data = subset_data(input_data,'all_sexes','all_ages','white_non_hispanic')
hist(input_data$real_income,breaks=100)
# mirror data
densities_old = estimate_densities(input_data,T)
plot(densities_old,main='Mirrored')
# re-weight data:
densities_new = estimate_densities(input_data,F)
plot(densities_new,main='Reweighted')
hist(input_data$real_income,breaks=1000)
hist(input_data$real_income,breaks=100)
hist(input_data$real_income,breaks=500)
hist(input_data$real_income,breaks=200)
expand.grid(c(1,2),c(1,4))
bird = c(1,2)
expand.grid(birds,c(1,4))
expand.grid(bird,c(1,4))
expand.grid(bird,c(1,4))[1]
expand.grid(bird,c(1,4))[1,]
expand.grid(bird,c(1,4))[1,][1]
expand.grid(bird,c(1,4))[1,][1][[1]]
expand.grid(bird,c(1,4))[1,]
expand.grid(bird,c(1,4))[1,][[1]]
expand.grid(bird,c(1,4))[1,]
unbox(expand.grid(bird,c(1,4))[1,])
as.vector(expand.grid(bird,c(1,4))[1,])
unname(expand.grid(bird,c(1,4))[1,])
demographic_i = c(1,2,3)
demographic_i
paste(demographic_i,'/')
paste0(demographic_i,'/')
a ]= [1]
a = 1
a
source('~/Documents/electric_scatter/dev/projects/income-distributions/data_prep/make_percentiles_update.R')
source('~/Documents/electric_scatter/dev/projects/income-distributions/data_prep/make_percentiles_update.R')
a = expand.grid(c(1,2),c(3,4))
for (thingy in a) print(thingy)
demographic_combos = expand.grid(sex_vars,age_vars,race_vars)
for (thingy in demographic_combos) print(thingy)
demographic_combos
a = demographic_combos[1]
a
for (thingy in demographic_combos){
print('-- -- -- -- -- --- --')
print(thingy)
}
typeof(demographic_combos)
nrow(demograhpic_combos)
nrow(demographic_combos)
a = list(c=1)
a
demographic_combos[1]
prepare_data_and_save = function(input_d,subset_type){
# Generate all possible demographic combos to loop through
demographic_combos = expand.grid(sex_vars,age_vars,race_vars)
for (row_i in 1:nrow(demographic_combos)){
demographic_i = unname(demographic_combos[1,])
print(demographic_i)
# Path for the directory where the file lives
dir_string =  paste0(c('../processed_data', subset_type, demographic_i),'/')
# Subset and pepare data save as json
formatted_data = input_d %>%
subset_data(demographic_i) %>%
cache_densities_and_incomes(1971,1972) %>%
make_json_and_save(dir_string)
}
}
# Export
prepare_data_and_save(current_subset,'mine')
subset_sex = function(input_data,sex_string){
print(sex_string)
if (sex_string == 'all_sexes') return(input_data)
return(subset(input_data,SEX==sex_dict[sex_string]))
}
# Export
prepare_data_and_save(current_subset,'mine')
subset_sex = function(input_data,sex_string){
print(input_data)
print(sex_string)
if (sex_string == 'all_sexes') return(input_data)
return(subset(input_data,SEX==sex_dict[sex_string]))
}
# Export
prepare_data_and_save(current_subset,'mine')
subset_sex = function(input_data,sex_string){
print(input_data)
print(sex_string == 'sleijfweo')
if (sex_string == 'all_sexes') return(input_data)
return(subset(input_data,SEX==sex_dict[sex_string]))
}
# Export
prepare_data_and_save(current_subset,'mine')
subset_sex = function(input_data,sex_string){
print(input_data)
print(sex_string == 'all_sexes')
if (sex_string == 'all_sexes') return(input_data)
return(subset(input_data,SEX==sex_dict[sex_string]))
}
# Export
prepare_data_and_save(current_subset,'mine')
a = demographic_combos[1,]
demographic_combos[1,]
unname(demographic_combos[1,])
b = c(1,2,3)
b
as.vector(unname(demographic_combos[1,]))
unname(demographic_combos[1,])
unname(demographic_combos[1,])[1]
unname(demographic_combos[1,])[2]
unname(demographic_combos[1,])[1] == 'all_sexes'
unname(demographic_combos[1,])[1] == 'all_werliew'
demographic_combos = data.frame(expand.grid(sex_vars,age_vars,race_vars))
demographic_combos[1,]
demographic_combos[1,][1]
demographic_combos[1,][1] == 'alsdasd'
unname(demographic_combos[1,][1])
unname(demographic_combos[1,][1]) == 'sdkffsd'
as.character(unname(demographic_combos[1,][1]))
unname(demographic_combos[1,][1])
as.character(unname(demographic_combos[1,][1]))
a = c('wkerh','kweurr','werih')
a[1]
a[1] == 'sleirh'
a[1] == 'wkerh'
a = data.frame(c=c('a','b'))
a
a[1,]
unname(a[1,])
unname(a[1,][1])
demographic_combos
demographic_combos[1,]
demographic_combos[1,][1]
unname(demographic_combos[1,])
unname(demographic_combos[1,])[1]
as.character(unname(demographic_combos[1,])[1])
a = data.frame(b=c('d','e'),f=c('g','h'))
a
a[1,]
unname(a[1,])
unname(a[1,])[1]
paste(unname(a[1,])[1],'0')
as.vector(a[1,])
as.character(as.vector(a[1,]))
mydf <- data.frame("myvar1"=c("mystring","2"),"myvar2"=c("mystring","3"), stringsAsFactors=F)
as.character(mydf[1,])
mydf
as.character(a[1,])
a[a,]
a[1,]
mydf <- data.frame("myvar1"=c("mystring","2"),"myvar2"=c("mystring","3"), stringsAsFactors=F)
a = data.frame(b=c('d','e'),f=c('g','h'),stringsASFactors=F)
a[1,]
a = data.frame(b=c('d','e'),f=c('g','h'),stringsAsFactors=F)
a[1,]
as.character(a[1,])
# Generate all possible demographic combos to loop through
demographic_combos = data.frame(expand.grid(sex_vars,age_vars,race_vars),stringsAsFactors = F)
demographic_combos[1,]
as.character(demographic_combos[1,])
# Generate all possible demographic combos to loop through
demographic_combos = expand.grid(sex_vars,age_vars,race_vars,stringsAsFactors = F)
as.character(demographic_combos[1,])
prepare_data_and_save = function(input_d,subset_type){
# Generate all possible demographic combos to loop through
demographic_combos = expand.grid(sex_vars,age_vars,race_vars,stringsAsFactors = F)
for (row_i in 1:nrow(demographic_combos)){
demographic_i = as.character(demographic_combos[1,])
print(demographic_i)
# Path for the directory where the file lives
dir_string =  paste0(c('../processed_data', subset_type, demographic_i),'/')
# Subset and pepare data save as json
formatted_data = input_d %>%
subset_data(demographic_i) %>%
cache_densities_and_incomes(1971,1972) %>%
make_json_and_save(dir_string)
}
}
# Export
prepare_data_and_save(current_subset,'mine')
subset_sex = function(input_data,sex_string){
if (sex_string == 'all_sexes') return(input_data)
return(subset(input_data,SEX==sex_dict[sex_string]))
}
# Export
prepare_data_and_save(current_subset,'mine')
source('~/Documents/electric_scatter/dev/projects/income-distributions/data_prep/make_percentiles_update.R', echo=TRUE)
# Function that converts subsetted data to readable json
cache_densities_and_incomes = function(input_data,first_year,last_year){
percentile_vec = seq(0.02,0.98,0.02)
data_obj = list(
'incomes' = list(),
'means' = list(),
'densities' = list()
)
# Get the highest percentile across all years for the current subset (i.e., max x axis)
percentile_max = 0
# Get the highest plotted density across all years (i.e., max y axis)
densities_max = 0
# We pass both the incomes matrix and the transposed matrix to the front end for faster processing
incomes_matrix = matrix(NA,nrow=(last_year-first_year)+1,ncol = length(percentile_vec)+1)
# Loop through years
for (year_i in first_year:last_year){
print(paste("Running year",year_i))
iteration_counter = year_i - first_year
year_data = subset(input_data,YEAR==year_i)
# Get relevant info to save
income_percentiles = unname(wtd.quantile(year_data$real_income,weights=year_data$ASECWT,probs=percentile_vec))
income_densities = estimate_densities(year_data[,c('real_income','ASECWT')])
income_mean = wtd.mean(year_data$real_income, weights=year_data$ASECWT)
if (max(income_percentiles) > percentile_max) percentile_max = max(income_percentiles)
if (max(income_densities) > densities_max) densities_max = max(income_densities)
# Store in data obj
print(income_percentiles)
data_obj$incomes[[iteration_counter]] = income_percentiles
data_obj$means[[iteration_counter]] = unbox(income_mean)
data_obj$densities[[iteration_counter]] = income_densities
# Add incomes to income matrix
incomes_matrix[iteration_counter,] = c(income_percentiles,income_mean)
}
# transposed_incomes_matrix = t(incomes_matrix)
# transposed_incomes = list()
# for (row_i in 1:nrow(transposed_incomes_matrix)){
#   transposed_incomes[[row_i]] = list('name'=unbox(row_i-1),'vals' = transposed_incomes_matrix[row_i,])
# }
data_obj$x_max = unbox(percentile_max)
data_obj$y_max = unbox(densities_max)
data_obj$transposed_incomes = t(incomes_matrix)
return (data_obj)
}
# Export
prepare_data_and_save(current_subset,'mine')
data_obj = list(
'incomes' = list(),
'means' = list(),
'densities' = list()
)
data_obj[[1]] = c(1,2,3,4,5)
data_incomes$incomes[[1]] = c(1,2,3,4,5)
data_obj$incomes[[1]] = c(1,2,3,4,5)
a = list()
a[[1]]
a[[1]] = c(1,2)
a[[1]] = c(1,2,3,4,5)
b = list(incomes=list())
b$incomes
b$incomes[[1]] = c(1,2,3)
# Function that converts subsetted data to readable json
cache_densities_and_incomes = function(input_data,first_year,last_year){
percentile_vec = seq(0.02,0.98,0.02)
data_obj = list(
'incomes' = list(),
'means' = list(),
'densities' = list()
)
# Get the highest percentile across all years for the current subset (i.e., max x axis)
percentile_max = 0
# Get the highest plotted density across all years (i.e., max y axis)
densities_max = 0
# We pass both the incomes matrix and the transposed matrix to the front end for faster processing
incomes_matrix = matrix(NA,nrow=(last_year-first_year)+1,ncol = length(percentile_vec)+1)
# Loop through years
for (year_i in first_year:last_year){
print(paste("Running year",year_i))
iteration_counter = (year_i - first_year) + 1
year_data = subset(input_data,YEAR==year_i)
# Get relevant info to save
income_percentiles = unname(wtd.quantile(year_data$real_income,weights=year_data$ASECWT,probs=percentile_vec))
income_densities = estimate_densities(year_data[,c('real_income','ASECWT')])
income_mean = wtd.mean(year_data$real_income, weights=year_data$ASECWT)
if (max(income_percentiles) > percentile_max) percentile_max = max(income_percentiles)
if (max(income_densities) > densities_max) densities_max = max(income_densities)
# Store in data obj
data_obj$incomes[[iteration_counter]] = income_percentiles
data_obj$means[[iteration_counter]] = unbox(income_mean)
data_obj$densities[[iteration_counter]] = income_densities
# Add incomes to income matrix
incomes_matrix[iteration_counter,] = c(income_percentiles,income_mean)
}
# transposed_incomes_matrix = t(incomes_matrix)
# transposed_incomes = list()
# for (row_i in 1:nrow(transposed_incomes_matrix)){
#   transposed_incomes[[row_i]] = list('name'=unbox(row_i-1),'vals' = transposed_incomes_matrix[row_i,])
# }
data_obj$x_max = unbox(percentile_max)
data_obj$y_max = unbox(densities_max)
data_obj$transposed_incomes = t(incomes_matrix)
return (data_obj)
}
# Export
prepare_data_and_save(current_subset,'mine')
source('~/Documents/electric_scatter/dev/projects/income-distributions/data_prep/make_percentiles_update.R', echo=TRUE)
make_json_and_save = function(formatted_data,directory_string){
# Convert subsetted dataframe to JSON
json_obj = toJSON(formatted_data) # Convert to json
# If directories don't exist, create them
if (dir.exists(directory_string)==F) dir.create(directory_string,recursive = T)
# Add data.json (filename) to directory string
file_str = paste0(directory_string,'data.json')
# Write to directory
print(file_str)
write(json_obj, file_str)
}
# Export
prepare_data_and_save(current_subset,'mine')
