library(dplyr)
library(ipumsr)
library(plyr)
setwd("/Users/mhardy/Documents/electric_scatter/dev/projects/generation-distributions/data_prep")
# Bandwidth for kernels
bandwidth = 10000
# Actual ages are one less than the reported ages as participants report income during previous year
# Eg, age 19 in survey corresponds to an actual estimated age of 18
min_age = 19
max_age = 66
# T/F for redoing x axis max calculation
# Only need to do this if you're changing the plots or updating the ipums data
# Note that there are different x axis maxes for each worker subset
recompute_x_axis_maxes = F
# T/F for redoing data caching that's already been done
# i.e., should you overwrite json that already exist
overwrite_jsons = T
# -- -- -- -- -- -- -- -- -- -- --
# -- -- -- -- -- -- -- -- -- -- --
# -- -- - HELPER FUNCTIONS - -- --
# -- -- -- -- -- -- -- -- -- -- --
# -- -- -- -- -- -- -- -- -- -- --
get_average_weeks_worked = function(input){
if (input==1) return(7)  # 1 - 13 weeks
if (input==2) return(20) # 14-26 weeks
return(27) # 27+ weeks, don't take midpoint as all will be included
}
get_average_weeks_unemployed = function(input){
if (input==0) return(0)
if (input==1) return (2.5)  # 1-4 weeks
if (input==2) return (7.5) # 5 - 10 weeks
if (input==3) return(12.5)  # 11- 14 weeks
if (input==4) return(20.5) # 15 - 26 weeks
if (input==9) return(0) # 27+ weeks, don't take midpoint as all will be included
return(27)
}
get_generation = function(input){
for (gen_i in gen_list){
if (between(input,gen_cutoffs[gen_i][[1]][1],gen_cutoffs[gen_i][[1]][2])) return(gen_i)
}
return('other')
}
subset_generation = function(input_data,generation_string){
if (generation_string=='all_generations') return(input_data)
return(subset(input_data,generation==generation_string))
}
mirror_data = function(input_data){
data_to_mirror = subset(input_data,real_income < bandwidth)
data_to_mirror$real_income = data_to_mirror$real_income * -1
return(rbind(input_data,data_to_mirror))
}
epanechnikov_kernel = function(bandwidth,threshold,kernel_center,asecwt) {
weighted_difference = abs((threshold - kernel_center) / bandwidth)
if (weighted_difference >= 1) return(0)
return(asecwt * (0.75 * (1 - (weighted_difference^2)) / bandwidth))
}
estimate_densities = function(input_data,x_axis_max){
thresholds = seq(0,x_axis_max,by=2000)
# Initialize zero vector of thresholds
densities = rep(0.0,length(thresholds))
normalization_factor = sum(input_data$ASECWT)
plotting_data = input_data %>% mirror_data() %>% subset(real_income < x_axis_max + bandwidth)
# cut off the threhsolds above the max of the data
thresholds_to_loop = thresholds[thresholds < max(plotting_data$real_income) + bandwidth]
# Loop over the threhoslds, get the density for each, an then normalize
densities = sapply(thresholds_to_loop,get_threshold_density,data_for_estimation = plotting_data) / normalization_factor
# Append zeros for thresholds above the max in the data
densities = c(densities,rep(0,length(thresholds) - length(thresholds_to_loop)))
return(densities)
}
get_threshold_density = function(threshold,data_for_estimation){
# get the data that will influence the estimated density at the current subset
threshold_subset = subset(data_for_estimation, abs(real_income-threshold) < bandwidth)
# get the threshold density
if (nrow(threshold_subset) == 0) return(0)
return(sum(apply(threshold_subset,1,function(d) epanechnikov_kernel(bandwidth,threshold,d['real_income'],d['ASECWT']))))
}
make_json_and_save = function(formatted_data,directory_string){
# Convert subsetted dataframe to JSON
json_obj = toJSON(formatted_data,digits=21,auto_unbox=T)
# If directories don't exist, create them
if (!dir.exists(directory_string)) dir.create(directory_string,recursive = T)
# Make directory string and save
file_str = paste0(directory_string,'/data.json')
write(json_obj, file_str)
}
process_data_and_save = function(input_data,worker_type,min_age,max_age,redo_x_axis_estimation){
# Get the highest income value to compute densities for
x_axis_max_filename = paste0('x_axis_maxes/',worker_type,'_',min_age,'-',max_age,'.txt')
if (redo_x_axis_estimation) write_x_axis_max(input_data,worker_type,min_age,max_age,x_axis_max_filename)
x_axis_max = as.integer(readLines(x_axis_max_filename))
for (generation_i in c('all_generations',gen_list)){
# Path for the directory where the file lives
dir_path = paste("../processed_data",worker_type,generation_i,sep='/')
if (!overwrite_jsons & !file.exists(paste0(dir_path,'/data.json'))){
print(paste("--------- Skipping",worker_type,generation_i,"---------"))
next
}
print(paste("--------- Processing",worker_type,generation_i,"---------"))
input_data %>%
subset_generation(generation_i) %>%
cache_densities_and_incomes(min_age,max_age,x_axis_max) %>%
make_json_and_save(dir_path)
}
}
cache_densities_and_incomes = function(input_data,min_age,max_age,max_x_axis){
# Percentiles to be tracked in the front end
percentile_vec = seq(0.02,0.98,0.02)
data_obj = list('incomes' = list(),'densities' = list(),'x_maxes' = list(),'y_maxes' = list())
# Get the highest percentile (x axis max) and denisty (y axis max) across all ages for the current subset
percentile_max = 0
densities_max = 0
# We pass both the incomes matrix and the transposed matrix to the front end for faster processing
incomes_matrix = matrix(NA,nrow=0,ncol = length(percentile_vec)+1)
cached_ages = vector()
counter = 1
for (age_i in min_age:max_age){
age_data = subset(input_data,AGE==age_i)[,c('real_income','ASECWT')]
if (nrow(age_data) >= 100){
income_percentiles = unname(wtd.quantile(age_data$real_income,weights=age_data$ASECWT,probs=percentile_vec))
income_densities = estimate_densities(age_data,max_x_axis)
income_mean = wtd.mean(age_data$real_income,weights=age_data$ASECWT)
curr_percentile_max = max(income_percentiles)
percentile_max = max(percentile_max,curr_percentile_max)
curr_densities_max = max(income_densities)
densities_max = max(densities_max,curr_densities_max)
# Store in data obj
data_obj$incomes[[counter]] = c(income_percentiles,income_mean)
data_obj$densities[[counter]] = income_densities
data_obj$x_maxes[[age_i]] = curr_percentile_max
data_obj$y_maxes[[age_i]] = curr_densities_max
# Add incomes to income matrix
incomes_matrix = rbind(incomes_matrix,c(income_percentiles,income_mean))
# For tracking min and max age
cached_ages = c(cached_ages,age_i)
print(paste('Age',age_i, 'done'))
counter = counter + 1
} else{
print(paste("!!!! Insufficient data flag:",age_i))
}
}
# Add global properties to data_obj
data_obj$global_x_max = percentile_max
data_obj$global_y_max = densities_max
data_obj$transposed_incomes = t(incomes_matrix)
data_obj$min_age = min(cached_ages) - 1
data_obj$max_age = max(cached_ages) - 1
return (data_obj)
}
# Loops thorugh the demographic combos, finds the highest 98th percentile
# Returns the smallest thold at least 20000 over this (tholds go in increments of 2,000)
# Saves this to a text file in the x_axis_maxes directory
write_x_axis_max = function(input_data,worker_type,min_age,max_age,x_axis_max_filename){
print(paste(' ------------- Finding x maxes for',worker_type,'-------------'))
# Loop through all generations, find the 98th percentile for each, save the highest one
max_income_percentile = 0
for (gen_i in c('all_generations',gen_list)){
print(paste('Getting maxes for',gen_i))
max_income_percentile = input_data %>%
{if (gen_i != 'all_generations') subset(.,generation == gen_i) else .} %>%
# Have to specify these are from dplyr as it conflicts with plyr
dplyr::group_by(AGE) %>%
dplyr::summarize(
max_age_percentile = wtd.quantile(real_income,weights=ASECWT,probs=0.98)
) %>%
select(max_age_percentile) %>%
max(max_income_percentile)
}
# Get the smallest multiple of 2000 at least 20000 over outer_max_percentile
x_axis_max = round_any(max_income_percentile,2000,f=ceiling) + 20000
# save max thold to file
write(as.character(x_axis_max), x_axis_max_filename)
}
# -- -- -- -- -- -- -- -- -- -- --
# -- -- -- -- -- -- -- -- -- -- --
# Initialize variables and load data
# -- -- -- -- -- -- -- -- -- -- --
# -- -- -- -- -- -- -- -- -- -- --
# Don't mess too much here
gen_list = c('greatest','silent','boomers','gen_x','millennials')
gen_cutoffs = list(
'greatest' = c(1901,1927),
'silent' = c(1928, 1945),
'boomers' = c(1946,1964),
'gen_x' = c(1965,1980),
'millennials' = c(1981,1996))
data_filepath = "../../../../public_github/projects/income-distributions/data_prep/ipums_data/cps_00009.xml"
raw_data = data_filepath %>%
read_ipums_ddi() %>%
read_ipums_micro() %>%
# Remove negative income people
subset(INCTOT>=0) %>%
# Remove income not in universe
subset(INCTOT!=999999999) %>%
# remove income Missing
subset(INCTOT!=999999998) %>%
mutate(
CPI_19 = CPI99 * (1/min(CPI99)),
real_income = INCTOT * CPI_19,
birth_year = YEAR - AGE,
generation = sapply(birth_year,get_generation)
) %>%
subset(generation!='other') %>%
# Workers age (min_age-1) - (max_age-1)
# Subtract one year because workers reported income for the previous year
subset(AGE>=min_age & AGE<=max_age)
raw_data = raw_data %>%
# Worked 50-52 weeks last year
subset(WKSWORK2==6) %>%
# Worked full time last year
subset(FULLPART==1)
greatest = subset(raw_data,generation=='greatest' & AGE == 19)
hist(greatest$real_income)
hist(greatest$real_income)
greatest = subset(raw_data,generation=='greatest' & AGE == 19)
greatest = subset(raw_data,generation=='greatest' && AGE == 19)
unique(raw_data$generation)
greatest = subset(raw_data,generation=='silent' & AGE == 19)
hist(silent$real_income)
silent = subset(raw_data,generation=='silent' & AGE == 19)
hist(silent$real_income)
hist(silent$real_income,breaks=300)
hist(silent$real_income,breaks=100)
millenialls = subset(raw_data,generation=='millennials' & AGE == 19)
silent = subset(raw_data,generation=='silent' & AGE == 19)
millenialls = subset(raw_data,generation=='millennials' & AGE == 19)
hist(silent$real_income,breaks=100)
hist(millenialls$real_income,breaks=100)
silent = subset(silent,real_income < 300000)
millenialls = subset(millenialls,real_income < 300000)
hist(silent$real_income,breaks=100)
hist(millenialls$real_income,breaks=100)
hist(silent$real_income,breaks=100)
hist(millenialls$real_income,breaks=100)
max(silent$real_income)
data_filepath = "../../../../public_github/projects/income-distributions/data_prep/ipums_data/cps_00009.xml"
raw_data = data_filepath %>%
read_ipums_ddi() %>%
read_ipums_micro()
max(raw_data$YEAR)
setwd('/Users/mhardy/Documents/princeton_research/collective_intelligence/public_repo/experiment_1')
package_list = c('lme4','dplyr')
new_packages = package_list[!(package_list %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
library('lme4')
library('dplyr')
# -- -- -- -- -- -- --
# LOAD AND CLEAN DATA
# -- -- -- -- -- -- --
e1_data = 'e1_data.csv' %>%
read.csv() %>%
subset(failed=='f') %>%
subset(is_overflow == "False") %>%
# Remove participant 2790
# Person was an extra participant in SOC:W-U, generation 7, network replicaiton 2 (caused by backend error)
subset(participant_id != 2790) %>%
mutate(
is_practice = is_practice=='True',
generation = as.integer(generation),
condition = case_when(
condition == 'ASO:N-U' ~ 'Asocial no bias',
condition == 'ASO:W-U' ~ 'Asocial bias',
condition == 'SOC:N-U' ~ 'Social no bias',
TRUE ~ 'Social bias'
),
proportion_green = 1-proportion_blue,
chose_correct = as.logical(chose_correct),
chose_utility = as.logical(chose_utility),
is_cloned = generation == 0 & condition %in% c('Social no bias','Social bias'),
k_chose_correct = ifelse(proportion_green > 0.5, k_chose_green, 8 - k_chose_green),
condition_id = paste(condition,condition_replication, sep="_")
) %>%
subset(is_practice == F) %>%
subset(is_cloned==F)
e1_data = load_e1_data()
load_e1_data = function(){
return(
'e1_data.csv' %>%
read.csv() %>%
subset(failed=='f') %>%
subset(is_overflow == "False") %>%
# Remove participant 2790
# Person was an extra participant in SOC:W-U, generation 7, network replicaiton 2 (caused by backend error)
subset(participant_id != 2790) %>%
mutate(
is_practice = is_practice=='True',
generation = as.integer(generation),
condition = case_when(
condition == 'ASO:N-U' ~ 'Asocial no bias',
condition == 'ASO:W-U' ~ 'Asocial bias',
condition == 'SOC:N-U' ~ 'Social no bias',
TRUE ~ 'Social bias'
),
proportion_green = 1-proportion_blue,
chose_correct = as.logical(chose_correct),
chose_utility = as.logical(chose_utility),
is_cloned = generation == 0 & condition %in% c('Social no bias','Social bias'),
k_chose_correct = ifelse(proportion_green > 0.5, k_chose_green, 8 - k_chose_green),
condition_id = paste(condition,condition_replication, sep="_")
) %>%
subset(is_practice == F) %>%
subset(is_cloned==F)
)
}
e1_data = load_e1_data()
unique(e1_data$condition)
# Get condition means for reporting
e1_data %>%
group_by(condition) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
e1_data = load_e1_data()
unique(e1_data$condition)
# Get condition means for reporting
e1_data %>%
group_by(condition) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
unique(e1_data$condition)
e1_data %>%
group_by(condition) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
)
# Get condition means for reporting
e1_data %>%
group_by(generation) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
# Get condition means for reporting
e1_data %>%
group_by(generation) %>%
summarize(
m = mean(k_chose_utility)
)
library('dplyr')
# Get condition means for reporting
e1_data %>%
group_by(generation) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
detach(package:plyr)
# Get condition means for reporting
e1_data %>%
group_by(generation) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
library(dplyr)
# Get condition means for reporting
e1_data %>%
group_by(generation) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
library('dplyr')
# Get condition means for reporting
e1_data %>%
group_by(generation) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
e1_data %>%
group_by(generation)
e1_data %>%
group_by(generation) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
)
detach("package:utils", unload=TRUE)
detach("package:survival", unload=TRUE)
detach("package:stats", unload=TRUE)
detach("package:Matrix", unload=TRUE)
detach("package:methods", unload=TRUE)
detach("package:lme4", unload=TRUE)
detach("package:lattice", unload=TRUE)
detach("package:ipumsr", unload=TRUE)
detach("package:jsonlite", unload=TRUE)
detach("package:Hmisc", unload=TRUE)
detach("package:grDevices", unload=TRUE)
detach("package:graphics", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
detach("package:Formula", unload=TRUE)
detach("package:dplyr", unload=TRUE)
detach("package:datasets", unload=TRUE)
### --- Script for running the pre-registered regressions --- ###
### --- Link available at https://osf.io/yth5r/ --- ###
setwd('/Users/mhardy/Documents/princeton_research/collective_intelligence/public_repo/experiment_1')
package_list = c('lme4','dplyr')
new_packages = package_list[!(package_list %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
library('lme4')
library('dplyr')
library('lme4')
library('dplyr')
if(length(new_packages)) install.packages(new_packages)
new_packages = package_list[!(package_list %in% installed.packages()[,"Package"])]
package_list = c('lme4','dplyr')
setwd('/Users/mhardy/Documents/princeton_research/collective_intelligence/public_repo/experiment_1')
package_list = c('lme4','dplyr')
new_packages = package_list[!(package_list %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
library('lme4')
library('dplyr')
load_e1_data = function(){
return(
'e1_data.csv' %>%
read.csv() %>%
subset(failed=='f') %>%
subset(is_overflow == "False") %>%
# Remove participant 2790
# Person was an extra participant in SOC:W-U, generation 7, network replicaiton 2 (caused by backend error)
subset(participant_id != 2790) %>%
mutate(
is_practice = is_practice=='True',
generation = as.integer(generation),
condition = case_when(
condition == 'ASO:N-U' ~ 'Asocial control',
condition == 'ASO:W-U' ~ 'Asocial bias',
condition == 'SOC:N-U' ~ 'Social control',
TRUE ~ 'Social bias'
),
proportion_green = 1-proportion_blue,
chose_correct = as.logical(chose_correct),
chose_utility = as.logical(chose_utility),
is_cloned = generation == 0 & condition %in% c('Social no bias','Social bias'),
k_chose_correct = ifelse(proportion_green > 0.5, k_chose_green, 8 - k_chose_green),
condition_id = paste(condition,condition_replication, sep="_")
) %>%
subset(is_practice == F) %>%
subset(is_cloned==F)
)
}
e1_data = load_e1_data()
# Get condition means for reporting
e1_data %>%
group_by(generation) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
# Get condition means for reporting
e1_data %>%
group_by(condition) %>%
summarize(
bias = round(mean(chose_utility),3),
accuracy = round(mean(chose_correct),3)
) %>%
data.frame()
testy()
source('test.R')
testy()
b = function(){
print('yay')
}
c = function(d){
d()
}
c(b)
roundup_to <- function(x, to = 10, up = T){
if(up) round(.Machine$double.eps^0.5 + x/to)*to else round(x/to)*to
}
roundup_to(1200,1000)
roundup_to(1200,2000)
roundup_to(1200,1000)
roundup_to(1200,1000,up=T)
roundup_to(1220,1000,up=T)
roundup_to(1220,100,up=T)
roundup_to(1220,100,up=F)
roundup_to(1220,100,up=T)
seq(100,1000,100)
roundup_to(1234,to=1000)
roundup_to(1534,to=1000)
roundup_to(1500,to=1000)
round_to <- function(x, to = 10, up = T){
if(up) round(.Machine$double.eps^0.5 + x/to)*to else round(x/to)*to
}
round_to(1500,1000,F)
round_to(1500,1000,T)
round_to(1400,1000,T)
round_to(1499,1000,T)
round_to(1499,100,T)
round_to(1450,100,T)
.Machine$double.eps^0.5
source('~/Documents/electric_scatter/dev/projects/density-plots/data_prep/make_histogram_data.R')
source('~/Documents/electric_scatter/dev/projects/density-plots/data_prep/make_histogram_data.R')
# Don't mess too much here
gen_list = c('boomers','millennials')
c('boomers','millennials')
c(1,2)
a = c(1,2,3,4,5)
